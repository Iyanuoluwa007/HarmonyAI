{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeac0dd7-6966-438b-ae7a-8856d842e931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirmed paths\n",
    "csv_path   = \"data/spotify_dataset.csv\"\n",
    "json1_path = \"data/900k Definitive Spotify Dataset.json\"\n",
    "json2_path = \"data/final_milliondataset_BERT_500K_revised.json\"\n",
    "\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Output dirs\n",
    "Path(\"data/interim\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"app/artifacts\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeb25b4f-e425-4c50-840b-64656e8c7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path, n=None):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                # light repair attempt (handles escaped slashes)\n",
    "                try:\n",
    "                    rows.append(json.loads(line.replace(\"\\\\/\", \"/\")))\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if n and (i+1) >= n:\n",
    "                break\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def to_float(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip().lower().replace(\"db\", \"\").replace(\"−\",\"-\")\n",
    "    s = re.sub(\",\", \"\", s)\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", s)\n",
    "    return float(m.group(0)) if m else np.nan\n",
    "\n",
    "def parse_tempo(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    v = to_float(x)\n",
    "    if v is np.nan: return np.nan\n",
    "    # If given as 0-1 normalize to ~40–220 BPM\n",
    "    if 0 < v <= 1.0:\n",
    "        return 40.0 + v * (220.0 - 40.0)\n",
    "    return v\n",
    "\n",
    "def parse_loudness_db(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    v = to_float(x)\n",
    "    if v is np.nan: return np.nan\n",
    "    if 0 <= v <= 1.0:\n",
    "        return -30.0 + v * 30.0\n",
    "    return v\n",
    "\n",
    "def parse_length_mmss(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip()\n",
    "    m = re.match(r\"^\\s*(\\d{1,2}):(\\d{1,2})\\s*$\", s)\n",
    "    if m:\n",
    "        return int(m.group(1))*60 + int(m.group(2))\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "_MONTHS = {\n",
    "    \"january\":1,\"february\":2,\"march\":3,\"april\":4,\"may\":5,\"june\":6,\n",
    "    \"july\":7,\"august\":8,\"september\":9,\"october\":10,\"november\":11,\"december\":12\n",
    "}\n",
    "def parse_release_date(x):\n",
    "    if pd.isna(x): return pd.NaT\n",
    "    s = str(x).strip()\n",
    "    # ISO first\n",
    "    ts = pd.to_datetime(s, errors=\"coerce\")\n",
    "    if pd.notna(ts): return ts\n",
    "    # \"29th April 2013\"\n",
    "    s2 = re.sub(r\"(\\d+)(st|nd|rd|th)\", r\"\\1\", s.lower())\n",
    "    m = re.match(r\"^\\s*(\\d{1,2})\\s+([a-z]+)\\s+(\\d{4})\\s*$\", s2)\n",
    "    if m:\n",
    "        day = int(m.group(1)); mon = _MONTHS.get(m.group(2), None); year = int(m.group(3))\n",
    "        if mon:\n",
    "            try:\n",
    "                return pd.Timestamp(year=year, month=mon, day=day)\n",
    "            except:\n",
    "                return pd.NaT\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "def norm_key(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ef454-b72b-4370-b4e3-708d1dc0deb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_j2 = read_jsonl(json2_path)   # revised BERT 500K (primary)\n",
    "df_j1 = read_jsonl(json1_path)\n",
    "df_csv = pd.read_csv(csv_path, low_memory=False)\n",
    "\n",
    "print(\"json2:\", df_j2.shape)\n",
    "print(\"json1:\", df_j1.shape)\n",
    "print(\"csv  :\", df_csv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f3523-ce42-4362-ac0a-4a6a97292bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_map = {\n",
    "    \"Artist(s)\": \"artist\", \"artist\": \"artist\", \"artists\": \"artist\",\n",
    "    \"song\": \"track_name\", \"Song\": \"track_name\", \"track_name\": \"track_name\",\n",
    "    \"text\": \"lyrics\", \"Lyrics\": \"lyrics\",\n",
    "    \"Genre\": \"genre\", \"genre\": \"genre\",\n",
    "    \"Album\": \"album\", \"album\": \"album\",\n",
    "    \"ISRC\": \"isrc\", \"isrc\": \"isrc\",\n",
    "    \"Release Date\": \"release_date\", \"release_date\": \"release_date\",\n",
    "    \"Key\": \"key\", \"key\": \"key\",\n",
    "    \"Time signature\": \"time_signature\", \"time_signature\": \"time_signature\",\n",
    "    \"Length\": \"length\", \"length\": \"length\",\n",
    "    \"Explicit\": \"explicit\", \"explicit\": \"explicit\",\n",
    "    \"emotion\": \"emotion\",\n",
    "    \"Tempo\": \"tempo\", \"tempo\": \"tempo\",\n",
    "    \"Loudness (db)\": \"loudness_db\", \"loudness\": \"loudness_db\",\n",
    "    \"Energy\": \"energy\", \"energy\": \"energy\",\n",
    "    \"Danceability\": \"danceability\", \"danceability\": \"danceability\",\n",
    "    \"Positiveness\": \"valence\", \"valence\": \"valence\",\n",
    "    \"Speechiness\": \"speechiness\", \"speechiness\": \"speechiness\",\n",
    "    \"Liveness\": \"liveness\", \"liveness\": \"liveness\",\n",
    "    \"Acousticness\": \"acousticness\", \"acousticness\": \"acousticness\",\n",
    "    \"Instrumentalness\": \"instrumentalness\", \"instrumentalness\": \"instrumentalness\",\n",
    "    \"Popularity\": \"popularity\", \"popularity\": \"popularity\",\n",
    "}\n",
    "\n",
    "def rename_keep(df):\n",
    "    df = df.rename(columns={c: common_map.get(c, c) for c in df.columns})\n",
    "    keep = [\n",
    "        \"isrc\",\"artist\",\"track_name\",\"lyrics\",\"genre\",\"album\",\"release_date\",\"key\",\n",
    "        \"time_signature\",\"length\",\"explicit\",\"emotion\",\"tempo\",\"loudness_db\",\n",
    "        \"energy\",\"danceability\",\"valence\",\"speechiness\",\"liveness\",\"acousticness\",\n",
    "        \"instrumentalness\",\"popularity\"\n",
    "    ]\n",
    "    return df[[c for c in keep if c in df.columns]].copy()\n",
    "\n",
    "df_j2 = rename_keep(df_j2)\n",
    "df_j1 = rename_keep(df_j1)\n",
    "df_csv = rename_keep(df_csv)\n",
    "\n",
    "df_j2.head(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06ce7f7-a353-4a11-826d-3d48633b8ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((551443, 24), (498052, 23), (551443, 23))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def clean_df(df):\n",
    "    if \"tempo\" in df:          df[\"tempo\"] = df[\"tempo\"].apply(parse_tempo)\n",
    "    if \"loudness_db\" in df:    df[\"loudness_db\"] = df[\"loudness_db\"].apply(parse_loudness_db)\n",
    "    if \"length\" in df:         df[\"length_s\"] = df[\"length\"].apply(parse_length_mmss)\n",
    "    if \"release_date\" in df:   df[\"release_date\"] = df[\"release_date\"].apply(parse_release_date)\n",
    "    # numeric ints that came as strings\n",
    "    for col in [\"energy\",\"danceability\",\"valence\",\"speechiness\",\"liveness\",\n",
    "                \"acousticness\",\"instrumentalness\",\"popularity\"]:\n",
    "        if col in df:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    # explicit to bool\n",
    "    if \"explicit\" in df:\n",
    "        df[\"explicit\"] = df[\"explicit\"].astype(str).str.strip().str.lower().map(\n",
    "            {\"true\": True, \"1\": True, \"yes\": True, \"y\": True, \"explicit\": True,\n",
    "             \"false\": False, \"0\": False, \"no\": False, \"n\": False}).fillna(np.nan)\n",
    "\n",
    "    # normalized join key\n",
    "    for c in [\"artist\",\"track_name\"]:\n",
    "        if c in df:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "    df[\"join_key\"] = (df.get(\"artist\",\"\").apply(norm_key) + \" — \" +\n",
    "                      df.get(\"track_name\",\"\").apply(norm_key))\n",
    "    # prefer isrc uppercase\n",
    "    if \"isrc\" in df:\n",
    "        df[\"isrc\"] = df[\"isrc\"].astype(str).str.strip().str.upper()\n",
    "    return df\n",
    "\n",
    "df_j2 = clean_df(df_j2)\n",
    "df_j1 = clean_df(df_j1)\n",
    "df_csv = clean_df(df_csv)\n",
    "\n",
    "df_j2.shape, df_j1.shape, df_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b0167f-1b02-43f4-b858-1018b2fac971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445069, 23)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose base\n",
    "base = df_j2.copy()\n",
    "\n",
    "# helper: left-prefer combine\n",
    "def coalesce(left, right, cols):\n",
    "    out = left.copy()\n",
    "    for c in cols:\n",
    "        if c in right.columns:\n",
    "            if c not in out.columns:\n",
    "                out[c] = np.nan\n",
    "            out[c] = out[c].combine_first(right[c])\n",
    "    return out\n",
    "\n",
    "# first join on ISRC where available\n",
    "def smart_merge(left, right):\n",
    "    left = left.copy()\n",
    "    right = right.copy()\n",
    "\n",
    "    # split rows with and without isrc\n",
    "    l_isrc = left[left[\"isrc\"].notna()] if \"isrc\" in left else left.iloc[0:0]\n",
    "    l_no   = left[left[\"isrc\"].isna()]  if \"isrc\" in left else left\n",
    "\n",
    "    r_isrc = right[right[\"isrc\"].notna()] if \"isrc\" in right else right.iloc[0:0]\n",
    "    r_no   = right[right[\"isrc\"].isna()]  if \"isrc\" in right else right\n",
    "\n",
    "    # 1) isrc join\n",
    "    if len(l_isrc) and len(r_isrc):\n",
    "        m1 = pd.merge(l_isrc, r_isrc, on=\"isrc\", how=\"left\", suffixes=(\"\", \"_r\"))\n",
    "    else:\n",
    "        m1 = l_isrc\n",
    "\n",
    "    # 2) join-key join for those without isrc\n",
    "    if len(l_no) and len(r_no):\n",
    "        m2 = pd.merge(l_no, r_no, on=\"join_key\", how=\"left\", suffixes=(\"\", \"_r\"))\n",
    "    else:\n",
    "        m2 = l_no\n",
    "\n",
    "    merged = pd.concat([m1, m2], ignore_index=True)\n",
    "\n",
    "    # coalesce _r columns\n",
    "    all_cols = set(keep_cols + [\"length_s\",\"join_key\",\"isrc\"])\n",
    "    for c in list(all_cols):\n",
    "        c_r = c + \"_r\"\n",
    "        if c in merged.columns and c_r in merged.columns:\n",
    "            merged[c] = merged[c].combine_first(merged[c_r])\n",
    "            merged.drop(columns=[c_r], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # drop any residual *_r\n",
    "    merged = merged[[c for c in merged.columns if not c.endswith(\"_r\")]]\n",
    "    return merged\n",
    "\n",
    "# merge json1 then csv\n",
    "merged = smart_merge(base, df_j1)\n",
    "merged = smart_merge(merged, df_csv)\n",
    "\n",
    "# final column order\n",
    "final_cols = [\n",
    "    \"isrc\",\"artist\",\"track_name\",\"album\",\"genre\",\"lyrics\",\"emotion\",\n",
    "    \"release_date\",\"key\",\"time_signature\",\"length_s\",\"explicit\",\n",
    "    \"tempo\",\"loudness_db\",\"energy\",\"danceability\",\"valence\",\"speechiness\",\n",
    "    \"liveness\",\"acousticness\",\"instrumentalness\",\"popularity\",\"join_key\"\n",
    "]\n",
    "final = merged[[c for c in final_cols if c in merged.columns]].copy()\n",
    "\n",
    "# deduplicate by ISRC first, then by join_key\n",
    "if \"isrc\" in final:\n",
    "    final = final.sort_values([\"isrc\",\"release_date\"], na_position=\"last\").drop_duplicates(subset=[\"isrc\"], keep=\"first\")\n",
    "final = final.sort_values([\"join_key\",\"release_date\"], na_position=\"last\").drop_duplicates(subset=[\"join_key\"], keep=\"first\")\n",
    "\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9434da44-3db0-4b44-a87d-079949748140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved without Parquet:\n",
      "  PKL: data\\interim\\spotify_clean.pkl | data\\interim\\spotify_50k.pkl\n",
      "  CSV: data\\interim\\spotify_clean.csv | data\\interim\\spotify_50k.csv\n",
      "Shapes: (445069, 23) (50000, 23)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# paths\n",
    "full_pkl   = Path(\"data/interim/spotify_clean.pkl\")\n",
    "subset_pkl = Path(\"data/interim/spotify_50k.pkl\")\n",
    "full_csv   = Path(\"data/interim/spotify_clean.csv\")\n",
    "subset_csv = Path(\"data/interim/spotify_50k.csv\")\n",
    "\n",
    "# ensure dir exists\n",
    "full_pkl.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# make subset\n",
    "n_subset = min(50000, len(final))\n",
    "subset = final.sample(n_subset, random_state=42)\n",
    "\n",
    "# save (engine-free)\n",
    "final.to_pickle(full_pkl)\n",
    "subset.to_pickle(subset_pkl)\n",
    "final.to_csv(full_csv, index=False)\n",
    "subset.to_csv(subset_csv, index=False)\n",
    "\n",
    "print(\" Saved without Parquet:\")\n",
    "print(\"  PKL:\", full_pkl, \"|\", subset_pkl)\n",
    "print(\"  CSV:\", full_csv,  \"|\", subset_csv)\n",
    "print(\"Shapes:\", final.shape, subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016af7d5-5e95-4469-9147-25228bf16e3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     14\u001b[0m subset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/interim/spotify_50k.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m# <- use full later: data/interim/spotify_clean.parquet\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mlen\u001b[39m(df), df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()[:\u001b[38;5;241m12\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\Gesture_Control\\lib\\site-packages\\pandas\\io\\parquet.py:651\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[0;32m    500\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    510\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m    654\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    657\u001b[0m         )\n",
      "File \u001b[1;32m~\\.conda\\envs\\Gesture_Control\\lib\\site-packages\\pandas\\io\\parquet.py:67\u001b[0m, in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     65\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# fast dev loop\n",
    "df = pd.read_pickle(\"data/interim/spotify_50k.pkl\")\n",
    "\n",
    "# later (full set)\n",
    "# df = pd.read_pickle(\"data/interim/spotify_clean.pkl\")\n",
    "\n",
    "\n",
    "# Load the 50k subset for fast iteration (switch to full later)\n",
    "import os, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "subset_path = \"data/interim/spotify_50k.parquet\"   # <- use full later: data/interim/spotify_clean.parquet\n",
    "df = pd.read_parquet(subset_path)\n",
    "len(df), df.columns.tolist()[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756154d-fbb1-458e-9a20-6b2f9c93c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lyric + meta embeddings with Sentence-BERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # fast & good\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# fallback text if lyrics missing\n",
    "texts_lyrics = df[\"lyrics\"].fillna(df[\"track_name\"] + \" \" + df[\"artist\"])\n",
    "texts_meta   = (df[\"artist\"].fillna(\"\") + \" | \"\n",
    "                + df.get(\"genre\", pd.Series([\"\"]*len(df))).fillna(\"\") + \" | \"\n",
    "                + df[\"track_name\"].fillna(\"\"))\n",
    "\n",
    "batch = 512\n",
    "\n",
    "def batched_encode(texts):\n",
    "    out = []\n",
    "    for i in tqdm(range(0, len(texts), batch)):\n",
    "        chunk = texts.iloc[i:i+batch].tolist()\n",
    "        E = model.encode(chunk, normalize_embeddings=True, show_progress_bar=False)\n",
    "        out.append(E)\n",
    "    return np.vstack(out).astype(\"float32\")\n",
    "\n",
    "lyrics_emb = batched_encode(texts_lyrics)\n",
    "meta_emb   = batched_encode(texts_meta)\n",
    "\n",
    "lyrics_emb.shape, meta_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88865caa-1a63-49af-b54c-52a590a18aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric features block (scaled)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols = [c for c in [\n",
    "    \"energy\",\"danceability\",\"valence\",\"speechiness\",\"liveness\",\"acousticness\",\n",
    "    \"instrumentalness\",\"popularity\",\"tempo\",\"loudness_db\",\"length_s\"\n",
    "] if c in df.columns]\n",
    "\n",
    "X_num = df[num_cols].fillna(df[num_cols].median())\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num).astype(\"float32\")\n",
    "X_num_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc4f00-172c-432c-83c8-041acfbe0a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion block (optional one-hot if present)\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "if \"emotion\" in df.columns:\n",
    "    emo_lists = df[\"emotion\"].fillna(\"\").astype(str).str.split(\",\").apply(\n",
    "        lambda xs: [x.strip().lower() for x in xs if x.strip()]\n",
    "    )\n",
    "    mlb = MultiLabelBinarizer(sparse_output=False)\n",
    "    emo_mat = mlb.fit_transform(emo_lists).astype(\"float32\")\n",
    "else:\n",
    "    emo_mat = np.zeros((len(df), 0), dtype=\"float32\")\n",
    "\n",
    "emo_mat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3ef60-c75b-4c24-adfc-c27c72814d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse into a single hybrid vector and L2-normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "w_lyrics, w_meta, w_num, w_emo = 0.6, 0.2, 0.15, 0.05\n",
    "\n",
    "# normalize numeric & emotion blocks first (safeguard)\n",
    "X_num_n = normalize(X_num_scaled) if X_num_scaled.shape[1] else X_num_scaled\n",
    "emo_n   = normalize(emo_mat) if emo_mat.shape[1] else emo_mat\n",
    "\n",
    "# combine: text channels are same dim; then append numeric/emotion channels\n",
    "text_block = (w_lyrics*lyrics_emb + w_meta*meta_emb)\n",
    "hybrid = np.hstack([text_block, w_num*X_num_n, w_emo*emo_n]).astype(\"float32\")\n",
    "hybrid = normalize(hybrid).astype(\"float32\")\n",
    "\n",
    "hybrid.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d7e21-672f-4631-8bed-492330b0ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts for reuse\n",
    "import joblib, os\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "np.save(\"data/processed/hybrid_emb_50k.npy\", hybrid)\n",
    "df_items = df[[\"artist\",\"track_name\",\"album\",\"genre\",\"release_date\"]].copy()\n",
    "df_items.to_parquet(\"data/processed/items_50k.parquet\", index=False)\n",
    "joblib.dump({\"scaler\": scaler, \"num_cols\": num_cols}, \"data/processed/prepro.pkl\")\n",
    "\n",
    "\"saved: hybrid_emb_50k.npy, items_50k.parquet, prepro.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531790a-693b-449f-9062-8ccfa4541d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss, numpy as np, os\n",
    "\n",
    "xb = np.load(\"data/processed/hybrid_emb_50k.npy\")\n",
    "d = xb.shape[1]\n",
    "index = faiss.IndexFlatIP(d)      # inner product == cosine if vectors are L2-normalized\n",
    "index.add(xb)\n",
    "faiss.write_index(index, \"data/processed/faiss_ip_50k.index\")\n",
    "d, index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e961c-1a3f-49b9-a297-3821982b6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out \"tail\" size (numeric + emotion dims)\n",
    "text_dim = lyrics_emb.shape[1]        # same as meta_emb.shape[1]\n",
    "tail_dim = hybrid.shape[1] - (text_dim + text_dim)  # rest is numeric+emotion\n",
    "\n",
    "import joblib\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class HybridQueryEncoder:\n",
    "    def __init__(self, model_name, w_lyrics=0.6, w_meta=0.2, tail_dim=0):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.w_lyrics = w_lyrics\n",
    "        self.w_meta = w_meta\n",
    "        self.tail_dim = tail_dim\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        E = self.model.encode([text], normalize_embeddings=True).astype(\"float32\")\n",
    "        q_text = self.w_lyrics*E + self.w_meta*E\n",
    "        tail = np.zeros((1, self.tail_dim), dtype=\"float32\")\n",
    "        q = np.hstack([q_text, tail])\n",
    "        return normalize(q).astype(\"float32\")\n",
    "\n",
    "enc = HybridQueryEncoder(model_name, w_lyrics=0.6, w_meta=0.2, tail_dim=tail_dim)\n",
    "joblib.dump(enc, \"data/processed/query_encoder.pkl\"), tail_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad9e8c-bee7-411f-818c-26f04bc20505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, faiss, joblib\n",
    "\n",
    "items = pd.read_parquet(\"data/processed/items_50k.parquet\")\n",
    "index = faiss.read_index(\"data/processed/faiss_ip_50k.index\")\n",
    "enc   = joblib.load(\"data/processed/query_encoder.pkl\")\n",
    "emb   = np.load(\"data/processed/hybrid_emb_50k.npy\")\n",
    "\n",
    "def rec_by_text(q, k=10):\n",
    "    qv = enc.encode(q)\n",
    "    scores, ids = index.search(qv, k)\n",
    "    out = items.iloc[ids[0]].copy()\n",
    "    out[\"score\"] = scores[0]\n",
    "    return out\n",
    "\n",
    "def rec_like_seed(seed_row_index, k=10):\n",
    "    qv = emb[seed_row_index:seed_row_index+1]\n",
    "    scores, ids = index.search(qv, k+1)\n",
    "    ids = [i for i in ids[0] if i != seed_row_index][:k]\n",
    "    out = items.iloc[ids].copy()\n",
    "    out[\"score\"] = (emb[ids] @ qv.T).ravel()\n",
    "    return out\n",
    "\n",
    "# Try it:\n",
    "display(rec_by_text(\"melancholic acoustic ballad\", 10))\n",
    "seed_idx = 0\n",
    "print(\"Seed:\", items.iloc[seed_idx].to_dict())\n",
    "display(rec_like_seed(seed_idx, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434ca36-cce3-4c93-9aa8-25a9b8be6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "os.makedirs(\"app/artifacts\", exist_ok=True)\n",
    "\n",
    "for fn in [\n",
    "    \"data/processed/faiss_ip_50k.index\",\n",
    "    \"data/processed/items_50k.parquet\",\n",
    "    \"data/processed/query_encoder.pkl\",\n",
    "    # optional: bring the embedding if you want same-like-seed search without FAISS\n",
    "    # \"data/processed/hybrid_emb_50k.npy\",\n",
    "]:\n",
    "    shutil.copy(fn, \"app/artifacts/\")\n",
    "\n",
    "\"artifacts copied to app/artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d1e50-a926-4187-9184-d76c5ad8d6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce3381-3c13-4d2c-bee0-b5572de9f032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Gesture_Control)",
   "language": "python",
   "name": "gesture_control"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
